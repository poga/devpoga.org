<!DOCTYPE html>
<html lang="en" dir="auto">

<head>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-K4HXJ8V');</script>


<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Using AntiPatterns to avoid MLOps Mistakes | Dev.Poga</title>
<meta name="keywords" content="machine learning, MLOps, patterns">
<meta name="description" content="Using AntiPatterns to avoid MLOps Mistakes, Nikil Muralidhar et. al.
I learned about this survey paper from The Data Exchange podcast. It&rsquo;s a good introduction for people who just started deploying their machine learning project to production.
The paper focused on supervised learning, and forcasting applications. But the observation and recommendations should be general enough to be applied to other common machine learning deployments.">
<meta name="author" content="poga">
<link rel="canonical" href="https://devpoga.org/blog/2021-11-27-mlops_antipatterns/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://devpoga.org/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://devpoga.org/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://devpoga.org/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://devpoga.org/apple-touch-icon.png">
<link rel="mask-icon" href="https://devpoga.org/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Using AntiPatterns to avoid MLOps Mistakes" />
<meta property="og:description" content="Using AntiPatterns to avoid MLOps Mistakes, Nikil Muralidhar et. al.
I learned about this survey paper from The Data Exchange podcast. It&rsquo;s a good introduction for people who just started deploying their machine learning project to production.
The paper focused on supervised learning, and forcasting applications. But the observation and recommendations should be general enough to be applied to other common machine learning deployments." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://devpoga.org/blog/2021-11-27-mlops_antipatterns/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2021-11-27T00:00:00+08:00" />
<meta property="article:modified_time" content="2021-11-27T00:00:00+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Using AntiPatterns to avoid MLOps Mistakes"/>
<meta name="twitter:description" content="Using AntiPatterns to avoid MLOps Mistakes, Nikil Muralidhar et. al.
I learned about this survey paper from The Data Exchange podcast. It&rsquo;s a good introduction for people who just started deploying their machine learning project to production.
The paper focused on supervised learning, and forcasting applications. But the observation and recommendations should be general enough to be applied to other common machine learning deployments."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "https://devpoga.org/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Using AntiPatterns to avoid MLOps Mistakes",
      "item": "https://devpoga.org/blog/2021-11-27-mlops_antipatterns/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Using AntiPatterns to avoid MLOps Mistakes",
  "name": "Using AntiPatterns to avoid MLOps Mistakes",
  "description": "Using AntiPatterns to avoid MLOps Mistakes, Nikil Muralidhar et. al.\nI learned about this survey paper from The Data Exchange podcast. It\u0026rsquo;s a good introduction for people who just started deploying their machine learning project to production.\nThe paper focused on supervised learning, and forcasting applications. But the observation and recommendations should be general enough to be applied to other common machine learning deployments.\n",
  "keywords": [
    "machine learning", "MLOps", "patterns"
  ],
  "articleBody": "Using AntiPatterns to avoid MLOps Mistakes, Nikil Muralidhar et. al.\nI learned about this survey paper from The Data Exchange podcast. It’s a good introduction for people who just started deploying their machine learning project to production.\nThe paper focused on supervised learning, and forcasting applications. But the observation and recommendations should be general enough to be applied to other common machine learning deployments.\nFrom the abstraction:\nFor the most part, we present our antipatterns in a supervised learning or forecasting context. In a production ML context, there is typically a model that has been approved for daily use. Over time, such a model might be replaced by a newer (e.g., more accurate) model, or retrained with more recent data (but keeping existing hyperparameters or ranges constant or fixed), or retrained with new search for hyperparameters in addition to retraining with recent data.\nPatterns \u0026 Anti-patterns We humans learn and communicate in patterns. We rely on patterns to create a common vocabulary for effective collabration. The (in)famous Design Pattern, no matter you like it or not, saved us lot of time with vocabularies such as Factory Pattern.\nIn similar vein, the paper is also trying to name a set of patterns emerged from the real-world machine learning deployments.\nlessons learned from developing and deploying machine learning models at scale across the enterprise in a range of financial analytics applications. These lessons are presented in the form of antipatterns\nHere are the patterns listed in the paper. Check the origin paper if you want to learn about the detail.\nDesign \u0026 Development Anti-patterns Data Leakage make use of information that is not supposed to have in production.\ncommon examples are:\nLags between reporting the data and the actual measurement incorrect sampling method to create training and test set oversampling BEFORE splitting the training and test test pre-processing or normalization BEFORE splitting the training and test set Tuning under the Carpet Hyper-parameters are often extremely under-documented.\nAs hyper-parameters play such a crucial role in learning, it is imperative that the part of a learning pipeline concerned with hyper-parameter optimization be explicitly and painstakingly documented so as to be reproducible and easily adaptable.\nPerformance Evaluation Anti-patterns PEST: Perceived Empirical SuperioriTy Keep it simple and stupid before you start trying specific methododologies.\nIn our financial analytics context, we have found the KISS principle to encourage developers to try simple models first and to conduct an exhaustive comparison of models before advocating for specific methodologies. Recent benchmark pipelines like the GLUE and SQuAD benchmarks [30, 38] are potential ways to address the PEST antipattern.\nBad Credit Assignment Failed to understand what makes your model work (or not work). This sounds easy. But it’s actually really hard when your machine learning project is a pipeline with 100+ stages.\nthe failure to appropriately identify the source of performance gains in a modeling pipeline\nGrade your own Exam After working on the same project for a while, the machine learning practitioner naturally learned about some feature of the test set and apply the knowledge to the training process without knowing.\nTo avoid this antipattern, testing and evaluation data should be sampled independently, and for a robust performance analysis, should be kept hidden until model development is complete and must be used only for final valuation. In practice, it is not uncommon for model developers to have access to the final test set and by repeated testing against this known test set, modify their model accordingly to improve performance on the known test set. This practice called HARKing (Hypothesizing After Results are Known)\nDeployment \u0026 Maintenance Anti-patterns Act Now, Reflect Never Never deploy a model straight to production without any monitoring or filtering. It’s going to be a nightmare.\npredictions are used as-is without any filtering, monitoring, reflection, or manual inspection.\nSet \u0026 Forget Old models suffer from concept drifts. If there’s only one metadata you can track, track the model creation date.\nDecision support systems governed by data-driven models are required to effectively handle concept drift and yield accurate decisions. The primary technique to handle concept drift is learning rule sets using techniques based on decision trees and other similar interpretable tree-based\nCommunicate with Ambivalence There’s always uncertainty in machine learning services. Make sure there’s a way to escalate the problem when the confidence is low.\nWe all hate services that block you because of some “algorithms” and leave you no chance asking for help. Don’t make the same mistake.\nMost ML pipelines are tuned to generate predictions but little attention is paid to ensure that the model can sufficiently communicate information about its own uncertainty\nData Crisis as a Service Cleaning and Preparing the Data Properly is really hard. De-identification requires careful inspection and monitoring.\nThe development of models using data manually extracted and hygiened without recording the extraction or hygiene steps leads to a massive data preparation challenge for later attempts to validate (or even deploy) ML models. This is often the result of ‘sensitive’ data that is selectively sanitized for the modelers by some third-party data steward organization that cannot adequately determine the risk associated with direct data access. The data preparation steps are effectively swept under the carpet and must be completely reinvented later, often with surprising impact on the models because the pipeline ends up producing different data\nRethinking ML Deployment This is how data scientists evaluation their model (naturally, focusing on the model):\nHowever, in real-world ML deployments, we should focus on correctness and operability of the whole pipeline. We should evaluate the whole learning and decision process against the business value instead of just the model.\nIn other words, define your project scope and KPI before starting a maching learning project.\nRecommendations From the paper:\nUse AntiPatterns presented here to document a model management process to avoid costly but routine mistakes in model development, deployment, and approval. Use assertions to track data quality across the enterprise. This is crucial since ML models can be so dependent on faulty or noisy data and suitable checks and balances can ensure a safe operating environment for ML algorithms. Document data lineage along with transformations to support creation of ‘audit trails’ so models can be situated back in time and in specific data slices for re-training or re-tuning. Use ensembles to maintain a palette of models including remedial and compensatory pipelines in the event of errors. Track model histories through the lifecycle of an application. Ensure human-in-the-loop operational capability at multiple levels. Use our model presented for rethinking ML deployment from Section 4 as a basis to support interventions and communication opportunities. ",
  "wordCount" : "1097",
  "inLanguage": "en",
  "datePublished": "2021-11-27T00:00:00+08:00",
  "dateModified": "2021-11-27T00:00:00+08:00",
  "author":{
    "@type": "Person",
    "name": "poga"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://devpoga.org/blog/2021-11-27-mlops_antipatterns/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Dev.Poga",
    "logo": {
      "@type": "ImageObject",
      "url": "https://devpoga.org/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
  
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-K4HXJ8V"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://devpoga.org/" accesskey="h" title="Dev.Poga (Alt + H)">Dev.Poga</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://devpoga.org/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://devpoga.org/">Home</a>&nbsp;»&nbsp;<a href="https://devpoga.org/blog/">Blogs</a></div>
    <h1 class="post-title">
      Using AntiPatterns to avoid MLOps Mistakes
    </h1>
    <div class="post-meta">&lt;span title=&#39;2021-11-27 00:00:00 &#43;0800 CST&#39;&gt;November 27, 2021&lt;/span&gt;&amp;nbsp;·&amp;nbsp;poga

</div>
  </header> 
  <div class="post-content"><p><a href="https://arxiv.org/abs/2107.00079">Using AntiPatterns to avoid MLOps Mistakes</a>, Nikil Muralidhar et. al.</p>
<p>I learned about this survey paper from <a href="https://thedataexchange.media/mlops-anti-patterns/">The Data Exchange</a> podcast. It&rsquo;s a good introduction for people who just started deploying their machine learning project to production.</p>
<p>The paper focused on <em>supervised learning</em>, and <em>forcasting applications</em>. But the observation and recommendations should be general enough to be applied to other common machine learning deployments.</p>
<p>From the abstraction:</p>
<blockquote>
<p>For the most part, we present our antipatterns in a supervised learning or forecasting context. In a production ML context, there is typically a model that has been approved for daily use. Over time, such a model might be replaced by a newer (e.g., more accurate) model, or retrained with more recent data (but keeping existing hyperparameters or ranges constant or fixed), or retrained with new search for hyperparameters in addition to retraining with recent data.</p>
</blockquote>
<h1 id="patterns--anti-patterns">Patterns &amp; Anti-patterns<a hidden class="anchor" aria-hidden="true" href="#patterns--anti-patterns">#</a></h1>
<p>We humans learn and communicate in patterns. We rely on patterns to create a common vocabulary for effective collabration. <a href="https://en.wikipedia.org/wiki/Design_Patterns">The (in)famous Design Pattern</a>, no matter you like it or not, saved us lot of time with vocabularies such as <em>Factory Pattern</em>.</p>
<p>In similar vein, the paper is also trying to name a set of patterns emerged from the real-world machine learning deployments.</p>
<blockquote>
<p>lessons learned from developing and deploying machine learning models at scale across the enterprise in a range of financial analytics applications. These lessons are presented in the form of antipatterns</p>
</blockquote>
<p>Here are the patterns listed in the paper. Check <a href="https://arxiv.org/abs/2107.00079">the origin paper</a> if you want to learn about the detail.</p>
<hr>
<h2 id="design--development-anti-patterns">Design &amp; Development Anti-patterns<a hidden class="anchor" aria-hidden="true" href="#design--development-anti-patterns">#</a></h2>
<h3 id="data-leakage">Data Leakage<a hidden class="anchor" aria-hidden="true" href="#data-leakage">#</a></h3>
<p>make use of information that is not supposed to have in production.</p>
<p>common examples are:</p>
<ul>
<li>Lags between reporting the data and the actual measurement</li>
<li>incorrect sampling method to create training and test set</li>
<li>oversampling BEFORE splitting the training and test test</li>
<li>pre-processing or normalization BEFORE splitting the training and test set</li>
</ul>
<h3 id="tuning-under-the-carpet">Tuning under the Carpet<a hidden class="anchor" aria-hidden="true" href="#tuning-under-the-carpet">#</a></h3>
<p>Hyper-parameters are often extremely under-documented.</p>
<blockquote>
<p>As hyper-parameters play such a crucial role in learning, it is imperative that the part of a learning pipeline concerned with hyper-parameter optimization be explicitly and painstakingly documented so as to be reproducible and easily adaptable.</p>
</blockquote>
<h2 id="performance-evaluation-anti-patterns">Performance Evaluation Anti-patterns<a hidden class="anchor" aria-hidden="true" href="#performance-evaluation-anti-patterns">#</a></h2>
<h3 id="pest-perceived-empirical-superiority">PEST: Perceived Empirical SuperioriTy<a hidden class="anchor" aria-hidden="true" href="#pest-perceived-empirical-superiority">#</a></h3>
<p>Keep it simple and stupid before you start trying specific methododologies.</p>
<blockquote>
<p>In our financial analytics context, we have found the KISS principle to encourage developers to try simple models first and to conduct an exhaustive comparison of models before advocating for specific methodologies. Recent benchmark pipelines like the GLUE and SQuAD benchmarks [30, 38] are potential ways to address the PEST antipattern.</p>
</blockquote>
<h3 id="bad-credit-assignment">Bad Credit Assignment<a hidden class="anchor" aria-hidden="true" href="#bad-credit-assignment">#</a></h3>
<p>Failed to understand what makes your model work (or not work). This sounds easy. But it&rsquo;s actually really hard when your machine learning project is a pipeline with 100+ stages.</p>
<blockquote>
<p>the failure to appropriately identify the source of performance gains in a modeling pipeline</p>
</blockquote>
<h3 id="grade-your-own-exam">Grade your own Exam<a hidden class="anchor" aria-hidden="true" href="#grade-your-own-exam">#</a></h3>
<p>After working on the same project for a while, the machine learning practitioner naturally learned about some feature of the test set and apply the knowledge to the training process without knowing.</p>
<blockquote>
<p>To avoid this antipattern, testing and evaluation data should be sampled independently, and for a robust performance analysis, should be kept hidden until model development is complete and must be used only for final valuation. In practice, it is not uncommon for model developers to have access to the final test set and by repeated testing against this known test set, modify their model accordingly to improve performance on the known test set. This practice called HARKing (Hypothesizing After Results are Known)</p>
</blockquote>
<hr>
<h2 id="deployment--maintenance-anti-patterns">Deployment &amp; Maintenance Anti-patterns<a hidden class="anchor" aria-hidden="true" href="#deployment--maintenance-anti-patterns">#</a></h2>
<h3 id="act-now-reflect-never">Act Now, Reflect Never<a hidden class="anchor" aria-hidden="true" href="#act-now-reflect-never">#</a></h3>
<p>Never deploy a model straight to production without any monitoring or filtering. It&rsquo;s going to be a nightmare.</p>
<blockquote>
<p>predictions are used as-is without any filtering, monitoring, reflection, or manual inspection.</p>
</blockquote>
<h3 id="set--forget">Set &amp; Forget<a hidden class="anchor" aria-hidden="true" href="#set--forget">#</a></h3>
<p>Old models suffer from concept drifts. <strong>If there&rsquo;s only one metadata you can track, track the model creation date</strong>.</p>
<blockquote>
<p>Decision support systems governed by data-driven models are required to effectively handle concept drift and yield accurate decisions. The primary technique to handle concept drift is learning rule sets using techniques based on decision trees and other similar interpretable tree-based</p>
</blockquote>
<h3 id="communicate-with-ambivalence">Communicate with Ambivalence<a hidden class="anchor" aria-hidden="true" href="#communicate-with-ambivalence">#</a></h3>
<p>There&rsquo;s always uncertainty in machine learning services. Make sure there&rsquo;s a way to escalate the problem when the confidence is low.</p>
<p>We all hate services that block you because of some &ldquo;algorithms&rdquo; and leave you no chance asking for help. Don&rsquo;t make the same mistake.</p>
<blockquote>
<p>Most ML pipelines are tuned to generate predictions but little attention is paid to ensure that the model can sufficiently communicate information about its own uncertainty</p>
</blockquote>
<h3 id="data-crisis-as-a-service">Data Crisis as a Service<a hidden class="anchor" aria-hidden="true" href="#data-crisis-as-a-service">#</a></h3>
<p>Cleaning and Preparing the Data <strong>Properly</strong> is really hard. De-identification requires careful inspection and monitoring.</p>
<blockquote>
<p>The development of models using data manually extracted and hygiened without recording the extraction or hygiene steps leads to a massive data preparation challenge for later attempts to validate (or even deploy) ML models. This is often the result of ‘sensitive’ data that is selectively sanitized for the modelers by some third-party data steward organization that cannot adequately determine the risk associated with direct data access. The data preparation steps are effectively swept under the carpet and must be completely reinvented later, often with surprising impact on the models because the pipeline ends up producing different data</p>
</blockquote>
<hr>
<h1 id="rethinking-ml-deployment">Rethinking ML Deployment<a hidden class="anchor" aria-hidden="true" href="#rethinking-ml-deployment">#</a></h1>
<p>This is how data scientists evaluation their model (naturally, focusing on the model):</p>
<p><img loading="lazy" src="./01.png" alt=""  />
</p>
<p>However, in real-world ML deployments, we should focus on correctness and operability of the whole pipeline. We should evaluate the whole learning and decision process against the business value instead of just the model.</p>
<p><img loading="lazy" src="./02.png" alt=""  />
</p>
<p><strong>In other words, define your project scope and KPI before starting a maching learning project.</strong></p>
<hr>
<h1 id="recommendations">Recommendations<a hidden class="anchor" aria-hidden="true" href="#recommendations">#</a></h1>
<p>From the paper:</p>
<ol>
<li>Use AntiPatterns presented here to document a model management process to avoid costly but routine mistakes in model development, deployment, and approval.</li>
<li>Use assertions to track data quality across the enterprise. This is crucial since ML models can be so dependent on faulty or noisy data and suitable checks and balances can ensure a safe operating environment for ML algorithms.</li>
<li>Document data lineage along with transformations to support creation of ‘audit trails’ so models can be situated back in time and in specific data slices for re-training or re-tuning.</li>
<li>Use ensembles to maintain a palette of models including remedial and compensatory pipelines in the event of errors. Track model histories through the lifecycle of an application.</li>
<li>Ensure human-in-the-loop operational capability at multiple levels. Use our model presented for rethinking ML deployment from Section 4 as a basis to support interventions and communication opportunities.</li>
</ol>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://devpoga.org/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="https://devpoga.org/tags/mlops/">MLOps</a></li>
      <li><a href="https://devpoga.org/tags/patterns/">patterns</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://devpoga.org/">Dev.Poga</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
